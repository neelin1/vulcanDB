import re
from typing import Dict, List, Any, Optional
import pandas as pd
from sqlalchemy import MetaData, Table, UniqueConstraint, select, text, exc
from sqlalchemy.engine import Connection, Engine
import logging
import pprint

from vulcan.utils.llm_helpers import TableTraitsWithName

logger = logging.getLogger(__name__)


def _build_table_lookup(
    metadata: MetaData,
    table_order: List[str],
    table_traits: List[TableTraitsWithName],
    dataframe: pd.DataFrame,
) -> Dict[str, Dict[str, Any]]:
    """Pre-compute and cache per-table metadata that will be used while
    inserting rows into the database.

    Returns a dict keyed by ``table_name`` containing:
        * ``table_obj`` - SQLAlchemy ``Table`` object
        * ``traits`` - corresponding ``TableTraitsWithName`` instance
        * ``col_map`` - ``Dict[db_col_name -> csv_col_name or None]``
          (``None`` means the value does not originate directly from the CSV,
          e.g. surrogate IDs or FK placeholders)
        * ``pk_cols`` - ``List[Column]`` primary-key columns (usually 1 item)
        * ``natural_key_col`` - ``Optional[str]`` natural-key column name for
          *1:n* tables (empty for *1:1* tables)
    """

    traits_lookup = {t.name: t for t in table_traits}

    lookup: Dict[str, Dict[str, Any]] = {}

    for tbl_name in table_order:
        tbl_obj: Table = metadata.tables[tbl_name]
        traits = traits_lookup.get(tbl_name)
        if traits is None:
            raise KeyError(f"No traits found for table '{tbl_name}'")

        # ---------------------------------------------------------------
        # Build column mapping
        # ---------------------------------------------------------------
        # Start with identical names (db_col -> db_col) and overwrite with
        # any explicit mappings defined in ``traits.mapping``.
        explicit_map = {m.table_col: m.raw_csv_col for m in traits.mapping}
        col_map: Dict[str, Optional[str]] = {}

        for col in tbl_obj.columns:
            if col.autoincrement and col.primary_key:
                # Surrogate IDs are generated by the database – no CSV source
                col_map[col.name] = None
                continue

            # Prefer explicit mapping if present, else assume identical names
            csv_col = explicit_map.get(col.name, col.name)
            if csv_col not in dataframe.columns:
                # Column comes from FK or is synthesised – mark as None so we
                # know it must be populated programmatically later.
                csv_col = None
            col_map[col.name] = csv_col

        # Identify natural-key column for 1:n tables (used to de-duplicate)
        natural_key_col: Optional[str] = None
        if traits.relation_to_raw == "1:n" and traits.one_to_n:
            natural_key_col = traits.one_to_n.natural_key_col
        elif traits.relation_to_raw == "1:1" and traits.one_to_n:
            logger.warning(
                f"Table {tbl_name} has 1:1 relation but a 1:n relation is defined in traits"
            )
        elif traits.relation_to_raw == "1:n" and not traits.one_to_n:
            raise ValueError(
                f"Table {tbl_name} has 1:n relation but no natural-key column defined"
            )

        lookup[tbl_name] = {
            "table_obj": tbl_obj,
            "traits": traits,
            "col_map": col_map,
            "pk_cols": list(tbl_obj.primary_key.columns),
            "natural_key_col": natural_key_col,
            "cache": {},  # filled in later for 1:n
            "stats": {"attempt": 0, "dropped": 0},
        }

    return lookup


def push_data_in_db(
    engine: Engine,
    dataframe: pd.DataFrame,
    table_order: List[str],
    table_traits: List[TableTraitsWithName],
) -> Dict[str, Dict[str, Any]]:
    metadata = MetaData()
    metadata.reflect(bind=engine)
    # connection = engine.connect()

    # Step 1: Prepare look-up information
    lookup = _build_table_lookup(metadata, table_order, table_traits, dataframe)

    pprint.pprint(lookup)

    # Step 2: Initialize 1:n caches (natural-key -> primary-key)
    with engine.connect() as tmp_connection:
        for tbl_name, info in lookup.items():
            is_one_to_n = info["traits"].relation_to_raw == "1:n"
            if not is_one_to_n:
                continue

            nk_col = info[
                "natural_key_col"
            ]  # only/all 1:n table should have this populated
            cache: Dict[Any, Any] = {}
            if nk_col:
                table = info["table_obj"]
                pk_cols = info["pk_cols"]
                if len(pk_cols) != 1:
                    raise ValueError(
                        f"Table {tbl_name} has {len(pk_cols)} primary key columns; only single-column PKs are supported"
                    )

                pk_col = pk_cols[0].name
                sel = select(table.c[nk_col], table.c[pk_col])
                for nk_val, pk_val in tmp_connection.execute(sel).fetchall():
                    cache[nk_val] = pk_val
            else:
                raise ValueError(
                    f"Warning: Table {tbl_name} has 1:n relation but no natural-key column defined"
                )
            info["cache"] = cache

    # Step 3: Insert Rows
    for idx, (_, row) in enumerate(dataframe.iterrows()):
        with engine.begin() as connection:
            row_data = row.to_dict()
            per_row_pk_memory: Dict[str, Any] = (
                {}
            )  # saves newly generated PKs for this row

            for tbl_name in table_order:
                info = lookup[tbl_name]
                traits = info["traits"]
                info["stats"]["attempt"] += 1

                # gather insert_data ------------------------------------------------
                insert_data: Dict[str, Any] = {}
                for db_col, csv_col in info["col_map"].items():
                    if csv_col is None:
                        insert_data[db_col] = None  # placeholder – may fill FK later
                    else:
                        insert_data[db_col] = row_data.get(csv_col)

                # resolve FKs from previously inserted parents ---------------------
                for col in info["table_obj"].columns:
                    if col.foreign_keys:
                        parent_tbl = list(col.foreign_keys)[0].column.table.name
                        if parent_tbl in per_row_pk_memory:
                            insert_data[col.name] = per_row_pk_memory[parent_tbl]

                # 1:n handling ------------------------------------------------------
                if traits.relation_to_raw == "1:n":
                    nk_col = info["natural_key_col"]
                    nk_val = insert_data[nk_col]
                    if nk_val is None:
                        logger.warning(
                            f"Row {idx}: missing natural‑key value for table {tbl_name}; dropped"
                        )
                        info["stats"]["dropped"] += 1
                        continue

                    cache = info["cache"]
                    if nk_val in cache:
                        # row already exists – reuse PK and store for dependents
                        per_row_pk_memory[tbl_name] = cache[nk_val]
                        continue  # nothing to insert

                # attempt insert ----------------------------------------------------
                try:
                    result = connection.execute(
                        info["table_obj"].insert().values(**insert_data)
                    )
                    if traits.relation_to_raw == "1:n":
                        if (
                            result.inserted_primary_key
                            and len(result.inserted_primary_key) > 0
                        ):
                            new_pk = result.inserted_primary_key[0]
                            cache[nk_val] = new_pk
                            per_row_pk_memory[tbl_name] = new_pk
                        else:
                            # Handle case where inserted_primary_key is None or empty,
                            raise ValueError(
                                f"Row {idx}: Could not retrieve primary key after inserting into 1:n table {tbl_name}."
                            )
                    elif (
                        result.inserted_primary_key
                        and len(result.inserted_primary_key) > 0
                    ):
                        per_row_pk_memory[tbl_name] = result.inserted_primary_key[0]
                except exc.IntegrityError as e:
                    logger.warning(
                        f"Row {idx}: integrity error inserting into {tbl_name}: {e.orig}; dropped"
                    )
                    info["stats"]["dropped"] += 1
                    # skip dependents for the rest of this row
                    break
                except exc.DataError as e:
                    logger.warning(
                        f"Row {idx}: data error inserting into {tbl_name}: {e.orig}; dropped"
                    )
                    info["stats"]["dropped"] += 1
                    break

    return lookup
